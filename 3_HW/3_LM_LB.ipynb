{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "k1gpzj4guo8e1riwj3om1k",
        "id": "DGYdsAIIyOzR"
      },
      "source": [
        "### N-gram language models or how to write scientific papers\n",
        "\n",
        "Мы обучим нашу языковую модель на статьх [ArXiv](http://arxiv.org/) и посмотрим, сможем ли мы сгенерировать похожую!\n",
        "\n",
        "![img](https://media.npr.org/assets/img/2013/12/10/istock-18586699-monkey-computer_brick-16e5064d3378a14e0e4c2da08857efe03c04695e-s800-c85.jpg)\n",
        "\n",
        "_data by neelshah18 from [here](https://www.kaggle.com/neelshah18/arxivdataset/)_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellId": "u8jdaiy68oib3jvr4k01",
        "id": "tVPW6r3ayOzT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellId": "0c76vnyl3zui9yhtkodgrlf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "id": "40dc-gvMyOzU",
        "outputId": "0260e7ab-fb96-4a22-daf0-80231db41f4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-06-29 13:21:15--  https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.70.18, 2620:100:6026:18::a27d:4612\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.70.18|:443... "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/0mulrothty5o8i8ud9gz2/arxivData.json.tar.gz?rlkey=n759u5qx2xpxxglmrl390vwvk&dl=1 [following]\n",
            "--2025-06-29 13:21:16--  https://www.dropbox.com/scl/fi/0mulrothty5o8i8ud9gz2/arxivData.json.tar.gz?rlkey=n759u5qx2xpxxglmrl390vwvk&dl=1\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc002842d918e60f9e869b6f305f.dl.dropboxusercontent.com/cd/0/inline/CsjkbFpWR0rHTpTuj6loc0TRuS9RgHE5APk3gbxS4j0CE_vlKQLXX4uAXekzxBXmH1ddNJP1viKgZBMHpGgtIecss3tfsAyEqCa-WExNRpwMiQNMyYTDzXwVqCBr3oyR3yE/file?dl=1# [following]\n",
            "--2025-06-29 13:21:16--  https://uc002842d918e60f9e869b6f305f.dl.dropboxusercontent.com/cd/0/inline/CsjkbFpWR0rHTpTuj6loc0TRuS9RgHE5APk3gbxS4j0CE_vlKQLXX4uAXekzxBXmH1ddNJP1viKgZBMHpGgtIecss3tfsAyEqCa-WExNRpwMiQNMyYTDzXwVqCBr3oyR3yE/file?dl=1\n",
            "Resolving uc002842d918e60f9e869b6f305f.dl.dropboxusercontent.com (uc002842d918e60f9e869b6f305f.dl.dropboxusercontent.com)... 162.125.70.15, 2620:100:6026:15::a27d:460f\n",
            "Connecting to uc002842d918e60f9e869b6f305f.dl.dropboxusercontent.com (uc002842d918e60f9e869b6f305f.dl.dropboxusercontent.com)|162.125.70.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Csjhl8ZM4CVRfyGlyooSvGsQvQPd46lNkjD6KmIlG-aUy0fLeOQM62T-A1ulgwlqrAwroZDuVtVXN8AJSoNTwn5_WALtNE5lOrZVTq_j5nmcQpc5WEZbxDBsmXyYlmWP039bgDQ_-n0eJt094JWRUxsNq4TK1jAItDb1UlLbnnZboxWNPx5oqQyygYiq-Bm0O0j2lJ_vag7mhwUt9xZ3uCU8RCLhiSCEaR7iOivU_l5szlul9VLEfo8A2YoYV95VjpwgdAcXLgJMd--u86qg8QmOZ_2Y36NwTVFk8DP1rRxTP43HAaasJNHXZT0upKYIV9ttIXF9M7PrrUpdkhPmQoURb2_Bz1LYurO-avnkS0mjOg/file?dl=1 [following]\n",
            "--2025-06-29 13:21:17--  https://uc002842d918e60f9e869b6f305f.dl.dropboxusercontent.com/cd/0/inline2/Csjhl8ZM4CVRfyGlyooSvGsQvQPd46lNkjD6KmIlG-aUy0fLeOQM62T-A1ulgwlqrAwroZDuVtVXN8AJSoNTwn5_WALtNE5lOrZVTq_j5nmcQpc5WEZbxDBsmXyYlmWP039bgDQ_-n0eJt094JWRUxsNq4TK1jAItDb1UlLbnnZboxWNPx5oqQyygYiq-Bm0O0j2lJ_vag7mhwUt9xZ3uCU8RCLhiSCEaR7iOivU_l5szlul9VLEfo8A2YoYV95VjpwgdAcXLgJMd--u86qg8QmOZ_2Y36NwTVFk8DP1rRxTP43HAaasJNHXZT0upKYIV9ttIXF9M7PrrUpdkhPmQoURb2_Bz1LYurO-avnkS0mjOg/file?dl=1\n",
            "Reusing existing connection to uc002842d918e60f9e869b6f305f.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18933283 (18M) [application/binary]\n",
            "Saving to: ‘arxivData.json.tar.gz’\n",
            "\n",
            "arxivData.json.tar. 100%[===================>]  18.06M  10.0MB/s    in 1.8s    \n",
            "\n",
            "2025-06-29 13:21:20 (10.0 MB/s) - ‘arxivData.json.tar.gz’ saved [18933283/18933283]\n",
            "\n",
            "arxivData.json\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>day</th>\n",
              "      <th>id</th>\n",
              "      <th>link</th>\n",
              "      <th>month</th>\n",
              "      <th>summary</th>\n",
              "      <th>tag</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30177</th>\n",
              "      <td>[{'name': 'Yu-An Chung'}, {'name': 'Wei-Hung W...</td>\n",
              "      <td>22</td>\n",
              "      <td>1711.08490v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>11</td>\n",
              "      <td>Deep neural networks have been investigated in...</td>\n",
              "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Learning Deep Representations of Medical Image...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3694</th>\n",
              "      <td>[{'name': 'Ke Li'}, {'name': 'Jitendra Malik'}]</td>\n",
              "      <td>1</td>\n",
              "      <td>1512.00442v3</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>12</td>\n",
              "      <td>Existing methods for retrieving k-nearest neig...</td>\n",
              "      <td>[{'term': 'cs.DS', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Fast k-Nearest Neighbour Search via Dynamic Co...</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5974</th>\n",
              "      <td>[{'name': 'Hubert Haoyang Duan'}]</td>\n",
              "      <td>3</td>\n",
              "      <td>1402.0459v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>2</td>\n",
              "      <td>From a fresh data science perspective, this th...</td>\n",
              "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Applying Supervised Learning Algorithms and a ...</td>\n",
              "      <td>2014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13671</th>\n",
              "      <td>[{'name': 'A. Ukil'}, {'name': 'J. Bernasconi'...</td>\n",
              "      <td>18</td>\n",
              "      <td>1503.05272v1</td>\n",
              "      <td>[{'rel': 'related', 'href': 'http://dx.doi.org...</td>\n",
              "      <td>3</td>\n",
              "      <td>IR or near-infrared (NIR) spectroscopy is a me...</td>\n",
              "      <td>[{'term': 'cs.NE', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Improved Calibration of Near-Infrared Spectra ...</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24731</th>\n",
              "      <td>[{'name': 'Julien Mairal'}, {'name': 'Francis ...</td>\n",
              "      <td>12</td>\n",
              "      <td>1411.3230v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>11</td>\n",
              "      <td>In recent years, a large amount of multi-disci...</td>\n",
              "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Sparse Modeling for Image and Vision Processing</td>\n",
              "      <td>2014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  author  day            id  \\\n",
              "30177  [{'name': 'Yu-An Chung'}, {'name': 'Wei-Hung W...   22  1711.08490v2   \n",
              "3694     [{'name': 'Ke Li'}, {'name': 'Jitendra Malik'}]    1  1512.00442v3   \n",
              "5974                   [{'name': 'Hubert Haoyang Duan'}]    3   1402.0459v1   \n",
              "13671  [{'name': 'A. Ukil'}, {'name': 'J. Bernasconi'...   18  1503.05272v1   \n",
              "24731  [{'name': 'Julien Mairal'}, {'name': 'Francis ...   12   1411.3230v2   \n",
              "\n",
              "                                                    link  month  \\\n",
              "30177  [{'rel': 'alternate', 'href': 'http://arxiv.or...     11   \n",
              "3694   [{'rel': 'alternate', 'href': 'http://arxiv.or...     12   \n",
              "5974   [{'rel': 'alternate', 'href': 'http://arxiv.or...      2   \n",
              "13671  [{'rel': 'related', 'href': 'http://dx.doi.org...      3   \n",
              "24731  [{'rel': 'alternate', 'href': 'http://arxiv.or...     11   \n",
              "\n",
              "                                                 summary  \\\n",
              "30177  Deep neural networks have been investigated in...   \n",
              "3694   Existing methods for retrieving k-nearest neig...   \n",
              "5974   From a fresh data science perspective, this th...   \n",
              "13671  IR or near-infrared (NIR) spectroscopy is a me...   \n",
              "24731  In recent years, a large amount of multi-disci...   \n",
              "\n",
              "                                                     tag  \\\n",
              "30177  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
              "3694   [{'term': 'cs.DS', 'scheme': 'http://arxiv.org...   \n",
              "5974   [{'term': 'cs.LG', 'scheme': 'http://arxiv.org...   \n",
              "13671  [{'term': 'cs.NE', 'scheme': 'http://arxiv.org...   \n",
              "24731  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
              "\n",
              "                                                   title  year  \n",
              "30177  Learning Deep Representations of Medical Image...  2017  \n",
              "3694   Fast k-Nearest Neighbour Search via Dynamic Co...  2015  \n",
              "5974   Applying Supervised Learning Algorithms and a ...  2014  \n",
              "13671  Improved Calibration of Near-Infrared Spectra ...  2015  \n",
              "24731    Sparse Modeling for Image and Vision Processing  2014  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!wget \"https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\" -O arxivData.json.tar.gz\n",
        "!tar -xvzf arxivData.json.tar.gz\n",
        "data = pd.read_json(\"./arxivData.json\")\n",
        "data.sample(n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellId": "lbyqb5rx7j8jpo591r06ak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC4Gp_9pyOzU",
        "outputId": "47ee23cb-47c6-4478-f707-577804747d15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Differential Contrastive Divergence ; This paper has been retracted.',\n",
              " 'What Does Artificial Life Tell Us About Death? ; Short philosophical essay',\n",
              " 'P=NP ; We claim to resolve the P=?NP problem via a formal argument for P=NP.']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# assemble lines: concatenate title and description\n",
        "lines = data.apply(lambda row: row['title'] + ' ; ' + row['summary'].replace(\"\\n\", ' '), axis=1).tolist()\n",
        "\n",
        "sorted(lines, key=len)[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "7u97m5s8ekl5zd5a43a1yc",
        "id": "koeSWsFtyOzU"
      },
      "source": [
        "### Токенизация\n",
        "\n",
        "Все как обычно, вы уже опытные. Данные кроме тебя никто не токенизирует. Займитесь очисткой данных. Используйте WordPunctTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellId": "u8rvfk719iek97t3rarwr",
        "id": "SbDUujAdyOzU"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "tokenizer = WordPunctTokenizer()\n",
        "\n",
        "for i, text in enumerate(lines):\n",
        "    text = text.lower()                      # сначала в нижний регистр\n",
        "    tokens = tokenizer.tokenize(text)          # токенизируем\n",
        "    lines[i] = ' '.join(tokens)              # записываем обратно\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellId": "w88nddpp2k8edoeyyyjh0l",
        "id": "wpOAF0IfyOzV"
      },
      "outputs": [],
      "source": [
        "assert sorted(lines, key=len)[0] == \\\n",
        "    'differential contrastive divergence ; this paper has been retracted .'\n",
        "assert sorted(lines, key=len)[2] == \\\n",
        "    'p = np ; we claim to resolve the p =? np problem via a formal argument for p = np .'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "qb6h3hxmr095egzv8rlzul",
        "id": "aGL9Ls4tyOzV"
      },
      "source": [
        "### N-граммовая языковая модель\n",
        "\n",
        "Языковая модель - это вероятностная модель, которая оценивает вероятность текста: совместную вероятность всех лексем $w_t$ в тексте $X$: $P(X) = P(w_1, \\dots, w_T)$.\n",
        "\n",
        "Это можно сделать, следуя правилу цепочки:\n",
        "$$P(w_1, \\dots, w_T) = P(w_1)P(w_2 \\mid w_1)\\dots P(w_T \\mid w_1, \\dots, w_{T-1}).$$\n",
        "\n",
        "Проблема такого подхода заключается в том, что финальный термин $P(w_T \\mid w_1, \\dots, w_{T-1})$ зависит от $n-1$ предыдущих слов. Эту вероятность нецелесообразно оценивать для длинных текстов, например, $T = 1000$.\n",
        "\n",
        "Одна из популярных аппроксимаций - предположить, что следующее слово зависит только от конечного количества предыдущих слов:\n",
        "\n",
        "\n",
        "$$P(w_t \\mid w_1, \\dots, w_{t - 1}) = P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1}).$$\n",
        "\n",
        "\n",
        "Такая модель называется __n-gram language model__, где n - параметр. Например, в 3-граммовой языковой модели каждое слово зависит только от двух предыдущих слов.\n",
        "\n",
        "$$\n",
        "    P(w_1, \\dots, w_n) = \\prod_t P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1}).\n",
        "$$\n",
        "\n",
        "Иногда такую аппроксимацию также можно встретить под названием _марковское предположение n-го порядка_.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "u68wydbiioqlp5gl96mhd",
        "id": "L2b40dxVyOzV"
      },
      "source": [
        "Первым этапом построения такой модели является подсчет всех вхождений слов с учетом N-1 предыдущих слов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellId": "og84gjipnumsakhiiu9ap",
        "id": "rcSMKRpFyOzV"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from collections import defaultdict, Counter, deque\n",
        "\n",
        "# special tokens:\n",
        "# - `UNK` represents absent tokens,\n",
        "# - `EOS` is a special token after the end of sequence\n",
        "\n",
        "UNK, EOS = \"_UNK_\", \"_EOS_\"\n",
        "\n",
        "def count_ngrams(lines, n):\n",
        "    \"\"\"\n",
        "    Count how many times each word occured after (n - 1) previous words\n",
        "    :param lines: an iterable of strings with space-separated tokens\n",
        "    :returns: a dictionary { tuple(prefix_tokens): {next_token_1: count_1, next_token_2: count_2}}\n",
        "\n",
        "    When building counts, please consider the following two edge cases:\n",
        "    - if prefix is shorter than (n - 1) tokens, it should be padded with UNK. For n=3,\n",
        "      empty prefix: \"\" -> (UNK, UNK)\n",
        "      short prefix: \"the\" -> (UNK, the)\n",
        "      long prefix: \"the new approach\" -> (new, approach)\n",
        "    - you should add a special token, EOS, at the end of each sequence\n",
        "      \"... with deep neural networks .\" -> (..., with, deep, neural, networks, ., EOS)\n",
        "      count the probability of this token just like all others.\n",
        "    \"\"\"\n",
        "    counts = defaultdict(Counter)\n",
        "    \n",
        "    for line in tqdm(lines, desc=f\"Counting {n}-grams\"):\n",
        "        # разбиваем строку на токены и добавляем EOS\n",
        "        tokens = line.split() + [EOS]\n",
        "        # инициализируем префикс UNK-ами\n",
        "        prefix = deque([UNK] * (n - 1), maxlen=n - 1)\n",
        "        \n",
        "        # для каждого токена инкрементируем счётчик\n",
        "        for tok in tokens:\n",
        "            counts[tuple(prefix)][tok] += 1\n",
        "            # сдвигаем префикс: добавляем текущий токен\n",
        "            if n > 1:\n",
        "                prefix.append(tok)\n",
        "    return counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellId": "xyf2he6lak9mmqarl3nck",
        "id": "jZPOcaJXyOzV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Counting 3-grams: 100%|██████████| 100/100 [00:00<00:00, 35481.80it/s]\n"
          ]
        }
      ],
      "source": [
        "# let's test it\n",
        "dummy_lines = sorted(lines, key=len)[:100]\n",
        "dummy_counts = count_ngrams(dummy_lines, n=3)\n",
        "assert set(map(len, dummy_counts.keys())) == {2}, \"please only count {n-1}-grams\"\n",
        "assert len(dummy_counts[('_UNK_', '_UNK_')]) == 78\n",
        "assert dummy_counts['_UNK_', 'a']['note'] == 3\n",
        "assert dummy_counts['p', '=']['np'] == 2\n",
        "assert dummy_counts['author', '.']['_EOS_'] == 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "4j620npeqvj0k8ak8xqx8xk",
        "id": "lT990fFOyOzV"
      },
      "source": [
        "Once we can count N-grams, we can build a probabilistic language model.\n",
        "The simplest way to compute probabilities is in proporiton to counts:\n",
        "\n",
        "$$ P(w_t | prefix) = { Count(prefix, w_t) \\over \\sum_{\\hat w} Count(prefix, \\hat w) } $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellId": "c7cm76wmzlaa12bctznzei",
        "id": "IFBR2dVLyOzV"
      },
      "outputs": [],
      "source": [
        "class NGramLanguageModel:\n",
        "    def __init__(self, lines, n):\n",
        "        \"\"\"\n",
        "        Train a simple count-based language model:\n",
        "        compute probabilities P(w_t | prefix) given ngram counts\n",
        "\n",
        "        :param n: computes probability of next token given (n - 1) previous words\n",
        "        :param lines: an iterable of strings with space-separated tokens\n",
        "        \"\"\"\n",
        "        assert n >= 1\n",
        "        self.n = n\n",
        "\n",
        "        counts = count_ngrams(lines, self.n)\n",
        "\n",
        "        # compute token proabilities given counts\n",
        "        self.probs = defaultdict(Counter)\n",
        "        # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n",
        "\n",
        "        # populate self.probs with actual probabilities\n",
        "        for prefix, next_counts in counts.items():\n",
        "            total = sum(next_counts.values())                  # общее число всех продолжений\n",
        "            for token, cnt in next_counts.items():\n",
        "                self.probs[prefix][token] = cnt / total\n",
        "\n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "        \"\"\"\n",
        "        :param prefix: string with space-separated prefix tokens\n",
        "        :returns: a dictionary {token : it's probability} for all tokens with positive probabilities\n",
        "        \"\"\"\n",
        "        prefix = prefix.split()\n",
        "        prefix = prefix[max(0, len(prefix) - self.n + 1):]\n",
        "        prefix = [ UNK ] * (self.n - 1 - len(prefix)) + prefix\n",
        "        return self.probs[tuple(prefix)]\n",
        "\n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        \"\"\"\n",
        "        :param prefix: string with space-separated prefix tokens\n",
        "        :param next_token: the next token to predict probability for\n",
        "        :returns: P(next_token|prefix) a single number, 0 <= P <= 1\n",
        "        \"\"\"\n",
        "        return self.get_possible_next_tokens(prefix).get(next_token, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "0ftnn4nmuzrup6c0vvhb8q",
        "id": "3NdGNXcwyOzW"
      },
      "source": [
        "Let's test it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellId": "a7zajcnvhqupvcrmacvkur",
        "id": "MKRW7HyLyOzW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Counting 3-grams: 100%|██████████| 100/100 [00:00<00:00, 10250.51it/s]\n"
          ]
        }
      ],
      "source": [
        "dummy_lm = NGramLanguageModel(dummy_lines, n=3)\n",
        "\n",
        "p_initial = dummy_lm.get_possible_next_tokens('') # '' -> ['_UNK_', '_UNK_']\n",
        "assert np.allclose(p_initial['learning'], 0.02)\n",
        "assert np.allclose(p_initial['a'], 0.13)\n",
        "assert np.allclose(p_initial.get('meow', 0), 0)\n",
        "assert np.allclose(sum(p_initial.values()), 1)\n",
        "\n",
        "p_a = dummy_lm.get_possible_next_tokens('a') # '' -> ['_UNK_', 'a']\n",
        "assert np.allclose(p_a['machine'], 0.15384615)\n",
        "assert np.allclose(p_a['note'], 0.23076923)\n",
        "assert np.allclose(p_a.get('the', 0), 0)\n",
        "assert np.allclose(sum(p_a.values()), 1)\n",
        "\n",
        "assert np.allclose(dummy_lm.get_possible_next_tokens('a note')['on'], 1)\n",
        "assert dummy_lm.get_possible_next_tokens('a machine') == \\\n",
        "    dummy_lm.get_possible_next_tokens(\"there have always been ghosts in a machine\"), \\\n",
        "    \"your 3-gram model should only depend on 2 previous words\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "oh8r9a41kuk4r51wra9",
        "id": "2zLk1Q_CyOzW"
      },
      "source": [
        "Now that you've got a working n-gram language model, let's see what sequences it can generate. But first, let's train it on the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellId": "f17xoejjppmooo2nopw4xo",
        "id": "8TSs8hnHyOzW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Counting 3-grams: 100%|██████████| 41000/41000 [00:06<00:00, 6765.83it/s]\n"
          ]
        }
      ],
      "source": [
        "lm = NGramLanguageModel(lines, n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "2kd9glwnkr470qc4bt7f1e",
        "id": "mq3ASkyLyOzW"
      },
      "source": [
        "Процесс генерации последовательностей... ну, он последовательный. Вы храните список лексем и итеративно добавляете следующую лексему путем выборки с вероятностями.\n",
        "\n",
        "$ X = [] $\n",
        "\n",
        "__forever:__\n",
        "* $w_{next} \\sim P(w_{next} | X)$\n",
        "* $X = concat(X, w_{next})$\n",
        "\n",
        "\n",
        "Вместо выборки с вероятностями можно также попробовать всегда брать наиболее вероятную лексему, выборку среди топ-K наиболее вероятных лексем или выборку с температурой. В последнем случае (температура) выборка производится из\n",
        "* $w_{next} \\sim {P(w_{next} | X) ^ {1 / \\tau} \\over \\sum_{\\hat w} P(\\hat w | X) ^ {1 / \\tau}}$.\n",
        "\n",
        "Где $\\tau > 0$ - температура модели. Если $\\tau << 1$, то более вероятные токены будут выбраны с еще большей вероятностью, а менее вероятные исчезнут."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cellId": "sgbatlm9vzb4z889fho7",
        "id": "RZSLFp8oyOzW"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def get_next_token(lm, prefix, temperature=1.0):\n",
        "    \"\"\"\n",
        "    return next token after prefix;\n",
        "    :param temperature: samples proportionally to lm probabilities ^ (1 / temperature)\n",
        "        if temperature == 0, always takes most likely token. Break ties arbitrarily.\n",
        "    \"\"\"\n",
        "    # 1) получаем словарь {token: P(token | prefix)}\n",
        "    probs = lm.get_possible_next_tokens(prefix)\n",
        "    \n",
        "    # если нет ни одного продолжения, возвращаем UNK\n",
        "    if not probs:\n",
        "        return UNK\n",
        "    \n",
        "    tokens, p = zip(*probs.items())  # разделяем токены и вероятности\n",
        "    \n",
        "    # 2) temperature == 0 → жадное решение\n",
        "    if temperature == 0:\n",
        "        # возвращаем токен с максимальной вероятностью\n",
        "        return max(probs, key=probs.get)\n",
        "    \n",
        "    # 3) корректируем вероятности температурой\n",
        "    #    new_p[i] = p[i]^(1/temperature)\n",
        "    adjusted = [prob ** (1.0 / temperature) for prob in p]\n",
        "    \n",
        "    # нормируем\n",
        "    total = sum(adjusted)\n",
        "    weights = [a / total for a in adjusted]\n",
        "    \n",
        "    # 4) семплируем один токен по весам\n",
        "    return random.choices(tokens, weights)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cellId": "98l40131wjtd5xbdm5b2nr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcNoEA1AyOzW",
        "outputId": "2bf388b7-682e-4481-83d1-e049aa51b540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looks nice!\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "test_freqs = Counter([get_next_token(lm, 'there have') for _ in range(10000)])\n",
        "assert 250 < test_freqs['not'] < 450\n",
        "assert 8500 < test_freqs['been'] < 9500\n",
        "assert 1 < test_freqs['lately'] < 200\n",
        "\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=1.0) for _ in range(10000)])\n",
        "assert 1500 < test_freqs['learning'] < 3000\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.5) for _ in range(10000)])\n",
        "assert 8000 < test_freqs['learning'] < 9000\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.0) for _ in range(10000)])\n",
        "assert test_freqs['learning'] == 10000\n",
        "\n",
        "print(\"Looks nice!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cellId": "1nnnycga61rijt6nd8zai",
        "id": "kzYCNpXiyOzX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "artificial neural networks have shown the application domains . finally , we use multiple examples to unleash the full 360 - degree polynomial threshold function of discrete dynamical systems and produces sharper and more stable performance in numerous cases of single body shape is perhaps the most widely used to test the hypothesis space . results : on the combination function in cases with a mapreduce implementation of each variable on another . this is illustrated in two diverse datasets without outliers . to develop opponent modeling or solver ) that belong to the availability of huge trees , gaussian graphical\n"
          ]
        }
      ],
      "source": [
        "prefix = 'artificial' # <- your ideas :)\n",
        "\n",
        "for i in range(100):\n",
        "    prefix += ' ' + get_next_token(lm, prefix)\n",
        "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
        "        break\n",
        "\n",
        "print(prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cellId": "pxyjsv3b7r8thdfxlgitl",
        "id": "Btp7K-TOyOzX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bridging the gap between the best of our knowledge , this article we study the problem of finding a match among these algorithms are based on the other hand , on the other . this approach is demonstrated by experiments on the other hand , we propose a novel method that is , the proposed approach is demonstrated in a wide range of applications , the proposed method can be used to discover the common assumption that the proposed approach has been shown to be able to generate a high - dimensional space ; ( 2 ) a new approach for the\n"
          ]
        }
      ],
      "source": [
        "prefix = 'bridging the' # <- more of your ideas\n",
        "\n",
        "for i in range(100):\n",
        "    prefix += ' ' + get_next_token(lm, prefix, temperature=0.5)\n",
        "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
        "        break\n",
        "\n",
        "print(prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "2n90bscmzfko0qnctp7ysc",
        "id": "Q5ZDXt8OyOzX"
      },
      "source": [
        "__Больше в лабе:__ nucleus sampling, top-k sampling, beam search(не для слабонервных)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "3gdmey7g8at5n5c5x4gayh",
        "id": "R0L1RWjQyOzX"
      },
      "source": [
        "### Оценка языковых моделей: perplexity (1 балл)\n",
        "\n",
        "_perplexity - это мера того, насколько хорошо ваша модель аппроксимирует истинное распределение вероятностей, лежащее в основе данных. Меньшая perplexity = лучше модель_.\n",
        "\n",
        "Чтобы вычислить недоумение на одном предложении, используйте:\n",
        "$$\n",
        "    {\\mathbb{P}}(w_1 \\dots w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_t P(w_t \\mid w_{t - n}, \\dots, w_{t - 1})\\right)^{-\\frac1N},\n",
        "$$\n",
        "\n",
        "\n",
        "На уровне корпуса текстов perplexity - это произведение вероятностей всех лексем во всех предложениях в степени $1/N$, где $N$ - _общая длина (в лексемах) всех предложений в корпусе текстов_.\n",
        "\n",
        "Это число может быстро стать слишком маленьким для точности float32/float64, поэтому мы рекомендуем сначала вычислить log-perplexity (из log-probabilities), а затем взять экспоненту."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "cellId": "5hp010xyzzb4vqewo1bhny",
        "id": "AeZrR7XhyOzX"
      },
      "outputs": [],
      "source": [
        "def perplexity(lm, lines, min_logprob=np.log(10 ** -50.)):\n",
        "    \"\"\"\n",
        "    :param lines: a list of strings with space-separated tokens\n",
        "    :param min_logprob: if log(P(w | ...)) is smaller than min_logprop, set it equal to min_logrob\n",
        "    :returns: corpora-level perplexity - a single scalar number from the formula above\n",
        "\n",
        "    Note: do not forget to compute P(w_first | empty) and P(eos | full_sequence)\n",
        "\n",
        "    PLEASE USE lm.get_next_token_prob and NOT lm.get_possible_next_tokens\n",
        "    \"\"\"\n",
        "    sum_logprob = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for line in lines:\n",
        "        tokens = line.split()\n",
        "        prefix = []  # текущий префикс (список последних n-1 токенов)\n",
        "\n",
        "        # считаем log-вероятности для каждого токена\n",
        "        for token in tokens:\n",
        "            prob = lm.get_next_token_prob(' '.join(prefix), token)\n",
        "            if prob > 0:\n",
        "                logp = np.log(prob)\n",
        "            else:\n",
        "                logp = min_logprob\n",
        "            # если logp ещё меньше минимума, то обрезаем\n",
        "            if logp < min_logprob:\n",
        "                logp = min_logprob\n",
        "\n",
        "            sum_logprob += logp\n",
        "            total_tokens += 1\n",
        "\n",
        "            # обновляем префикс\n",
        "            prefix.append(token)\n",
        "            if len(prefix) > lm.n - 1:\n",
        "                prefix = prefix[-(lm.n - 1):]\n",
        "\n",
        "        # учитываем EOS\n",
        "        prob_eos = lm.get_next_token_prob(' '.join(prefix), EOS)\n",
        "        if prob_eos > 0:\n",
        "            logp_eos = np.log(prob_eos)\n",
        "        else:\n",
        "            logp_eos = min_logprob\n",
        "        if logp_eos < min_logprob:\n",
        "            logp_eos = min_logprob\n",
        "\n",
        "        sum_logprob += logp_eos\n",
        "        total_tokens += 1\n",
        "\n",
        "    # перплексия: exp(−1/N * sum logp)\n",
        "    return np.exp(- sum_logprob / total_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "cellId": "8b689bobhkey04x7pabupj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBrGh3W0yOzX",
        "outputId": "a99734c8-f3aa-49ba-f235-293b9e97e46a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Counting 1-grams: 100%|██████████| 100/100 [00:00<00:00, 26170.24it/s]\n",
            "Counting 3-grams: 100%|██████████| 100/100 [00:00<00:00, 12645.63it/s]\n",
            "Counting 10-grams: 100%|██████████| 100/100 [00:00<00:00, 29238.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexities: ppx1=318.213 ppx3=1.520 ppx10=1.184\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "lm1 = NGramLanguageModel(dummy_lines, n=1)\n",
        "lm3 = NGramLanguageModel(dummy_lines, n=3)\n",
        "lm10 = NGramLanguageModel(dummy_lines, n=10)\n",
        "\n",
        "ppx1 = perplexity(lm1, dummy_lines)\n",
        "ppx3 = perplexity(lm3, dummy_lines)\n",
        "ppx10 = perplexity(lm10, dummy_lines)\n",
        "ppx_missing = perplexity(lm3, ['the jabberwock , with eyes of flame , '])  # thanks, L. Carrol\n",
        "\n",
        "print(\"Perplexities: ppx1=%.3f ppx3=%.3f ppx10=%.3f\" % (ppx1, ppx3, ppx10))\n",
        "\n",
        "assert all(0 < ppx < 500 for ppx in (ppx1, ppx3, ppx10)), \"perplexity should be non-negative and reasonably small\"\n",
        "assert ppx1 > ppx3 > ppx10, \"higher N models should overfit and \"\n",
        "assert np.isfinite(ppx_missing) and ppx_missing > 10 ** 6, \"missing words should have large but finite perplexity. \" \\\n",
        "    \" Make sure you use min_logprob right\"\n",
        "assert np.allclose([ppx1, ppx3, ppx10], (318.2132342216302, 1.5199996213739575, 1.1838145037901249))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "ypc4lks4vs1li908fqi8",
        "id": "zE6mU8ypyOzX"
      },
      "source": [
        "Давайте теперь измерим perplexity по факту:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "cellId": "tjnehsem2lmijkg2lto4w",
        "id": "O6gDt1KyyOzX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Counting 1-grams: 100%|██████████| 30750/30750 [00:01<00:00, 27537.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N = 1, Perplexity = 1832.23136\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Counting 2-grams: 100%|██████████| 30750/30750 [00:02<00:00, 13503.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N = 2, Perplexity = 85653987.28774\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Counting 3-grams: 100%|██████████| 30750/30750 [00:04<00:00, 6885.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N = 3, Perplexity = 61999196259043346743296.00000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_lines, test_lines = train_test_split(lines, test_size=0.25, random_state=42)\n",
        "\n",
        "for n in (1, 2, 3):\n",
        "    lm = NGramLanguageModel(n=n, lines=train_lines)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "38nfbfkpzgfxik8kccyt1l",
        "id": "hN2QkNQAyOzX"
      },
      "outputs": [],
      "source": [
        "# whoops, it just blew up :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "oopn2o57wxm9vbxzycytce",
        "id": "zo6vRGKMyOzY"
      },
      "source": [
        "### LM Smoothing\n",
        "\n",
        "Проблема нашей простой языковой модели заключается в том, что всякий раз, когда она встречает n-грамму, которую никогда раньше не видела, она присваивает ей вероятность 0. Каждый раз, когда это происходит, недоумение взрывается.\n",
        "\n",
        "Для борьбы с этой проблемой существует техника, называемая __сглаживанием__. Суть ее заключается в том, чтобы изменить подсчеты таким образом, чтобы не допустить слишком низкого значения вероятности. Простейшим алгоритмом здесь является аддитивное сглаживание (оно же [сглаживание Лапаса](https://en.wikipedia.org/wiki/Additive_smoothing)):\n",
        "\n",
        "$$ P(w_t | prefix) = { Count(prefix, w_t) + \\delta \\over \\sum_{\\hat w} (Count(prefix, \\hat w) + \\delta) } $$\n",
        "\n",
        "Если количество префиксов невелико, аддитивное сглаживание приведет вероятности к более равномерному распределению. Не стоит забывать, что суммирование в знаменателе идет по _всем словам в словаре_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "cellId": "ioh26rlov6g8l2ssj1c8pm",
        "id": "Z2zY2qSOyOzY"
      },
      "outputs": [],
      "source": [
        "class LaplaceLanguageModel(NGramLanguageModel):\n",
        "    \"\"\" this code is an example, no need to change anything \"\"\"\n",
        "    def __init__(self, lines, n, delta=1.0):\n",
        "        self.n = n\n",
        "        counts = count_ngrams(lines, self.n)\n",
        "        self.vocab = set(token for token_counts in counts.values() for token in token_counts)\n",
        "        self.probs = defaultdict(Counter)\n",
        "\n",
        "        for prefix in counts:\n",
        "            token_counts = counts[prefix]\n",
        "            total_count = sum(token_counts.values()) + delta * len(self.vocab)\n",
        "            self.probs[prefix] = {token: (token_counts[token] + delta) / total_count\n",
        "                                          for token in token_counts}\n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "        token_probs = super().get_possible_next_tokens(prefix)\n",
        "        missing_prob_total = 1.0 - sum(token_probs.values())\n",
        "        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
        "        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n",
        "\n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        token_probs = super().get_possible_next_tokens(prefix)\n",
        "        if next_token in token_probs:\n",
        "            return token_probs[next_token]\n",
        "        else:\n",
        "            missing_prob_total = 1.0 - sum(token_probs.values())\n",
        "            missing_prob_total = max(0, missing_prob_total) # prevent rounding errors\n",
        "            return missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "cellId": "3xvxkdxcmfqucruyt66mdc",
        "id": "PfkSs1qkyOzY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Counting 1-grams: 100%|██████████| 100/100 [00:00<00:00, 22357.70it/s]\n",
            "Counting 2-grams: 100%|██████████| 100/100 [00:00<00:00, 13341.93it/s]\n",
            "Counting 3-grams: 100%|██████████| 100/100 [00:00<00:00, 29044.42it/s]\n"
          ]
        }
      ],
      "source": [
        "#test that it's a valid probability model\n",
        "for n in (1, 2, 3):\n",
        "    dummy_lm = LaplaceLanguageModel(dummy_lines, n=n)\n",
        "    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "cellId": "j6zqa50koitjjri9ipd8ec",
        "id": "aSoZzBwOyOzY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Counting 1-grams: 100%|██████████| 30750/30750 [00:01<00:00, 26867.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N = 1, Perplexity = 1832.66878\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Counting 2-grams: 100%|██████████| 30750/30750 [00:02<00:00, 13471.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N = 2, Perplexity = 470.48021\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Counting 3-grams: 100%|██████████| 30750/30750 [00:04<00:00, 7289.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N = 3, Perplexity = 3679.44765\n"
          ]
        }
      ],
      "source": [
        "for n in (1, 2, 3):\n",
        "    lm = LaplaceLanguageModel(train_lines, n=n, delta=0.1)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "pjuqt30jcerwbz1ym9zv1",
        "id": "Mj1nN58iyOzY"
      },
      "outputs": [],
      "source": [
        "# optional: try to sample tokens from such a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "3b8s1y9uls4fosu3yp28gg",
        "id": "IcZ-M7mwyOzZ"
      },
      "source": [
        "### Сглаживание Кнезера-Нея (4 балла)\n",
        "\n",
        "Сглаживание Лапласа - простой, достаточно хороший метод, но точно не SOTA\n",
        "\n",
        "\n",
        "Ваша последняя задача в этом ноутбуке - реализовать сглаживание [Кнезера-Нея](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing).\n",
        "\n",
        "Оно может быть вычислено рекуррентно, для n>1:\n",
        "\n",
        "$$P_{kn}(w_t | prefix_{n-1}) = { \\max(0, Count(prefix_{n-1}, w_t) - \\delta) \\over \\sum_{\\hat w} Count(prefix_{n-1}, \\hat w)} + \\lambda_{prefix_{n-1}} \\cdot P_{kn}(w_t | prefix_{n-2})$$\n",
        "\n",
        "где\n",
        "- $prefix_{n-1}$ - кортеж из {n-1} предыдущих лексем\n",
        "- $lambda_{prefix_{n-1}}$ - константа нормализации, выбранная таким образом, чтобы вероятности складывались в 1\n",
        "- Униграмма $P_{kn}(w_t | prefix_{n-2})$ соответствует сглаживанию Кнезера-Нея для {N-1}-грамматической языковой модели.\n",
        "- Униграмма $P_{kn}(w_t)$ - это частный случай: насколько вероятно увидеть x_t в незнакомом контексте.\n",
        "\n",
        "Более подробные формулы см. на слайдах лекции или в вики.\n",
        "\n",
        "__Ваша задача__ состоит в том, чтобы\n",
        "- реализовать класс `KneserNeyLanguageModel`,\n",
        "- протестировать его на 1-3 грамматических языковых моделях\n",
        "- найти оптимальную (в пределах разумного) дельту сглаживания для 3-граммовой языковой модели со сглаживанием Кнесер-Нея"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "2ix7kzw02v30oye55322all",
        "id": "H6g96hOGyOzZ"
      },
      "outputs": [],
      "source": [
        "# специальные токены\n",
        "UNK, EOS = \"_UNK_\", \"_EOS_\"\n",
        "\n",
        "class KneserNeyLanguageModel(NGramLanguageModel):\n",
        "    \"\"\"\n",
        "    Интерполированный Kneser–Ney с абсолютным дисконтом delta.\n",
        "    \"\"\"\n",
        "    def __init__(self, lines, n, delta=1.0):\n",
        "        assert n >= 1\n",
        "        self.n = n\n",
        "        self.delta = delta\n",
        "\n",
        "        # 1) Собираем счётчики для всех порядков 1..n\n",
        "        #    count_ngrams(lines, k) возвращает dict(prefix_tuple)->Counter(next_token)\n",
        "        self.counts = {k: count_ngrams(lines, k) for k in range(1, n+1)}\n",
        "\n",
        "        # 2) Предвычисляем для каждого порядка:\n",
        "        #    totals[k][prefix] = sum_c C(prefix→*)\n",
        "        #    uniques[k][prefix] = |{ w : C(prefix→w)>0 }|\n",
        "        self.totals  = {}\n",
        "        self.uniques = {}\n",
        "        for k in range(1, n+1):\n",
        "            self.totals[k]  = {pref: sum(cnts.values())\n",
        "                               for pref, cnts in self.counts[k].items()}\n",
        "            self.uniques[k] = {pref: len(cnts)\n",
        "                               for pref, cnts in self.counts[k].items()}\n",
        "\n",
        "        # 3) continuation_counts: для каждого токена w — в скольких разных\n",
        "        #    префиксах (order=2) он встречался\n",
        "        if n >= 2:\n",
        "            bigrams = self.counts[2]\n",
        "            cont = Counter()\n",
        "            for pref, nxt in bigrams.items():\n",
        "                for w in nxt:\n",
        "                    cont[w] += 1\n",
        "            self.continuation_counts = cont\n",
        "            self.cont_total = sum(cont.values())\n",
        "        else:\n",
        "            # для униграмм используем просто raw-частоты\n",
        "            unigrams = self.counts[1]\n",
        "            cont = Counter()\n",
        "            for pref, nxt in unigrams.items():  # pref == ()\n",
        "                for w, c in nxt.items():\n",
        "                    cont[w] = c\n",
        "            self.continuation_counts = cont\n",
        "            self.cont_total = sum(cont.values())\n",
        "\n",
        "        # 4) Собираем полный словарь (vocab) всех токенов\n",
        "        #    Здесь все, что хоть раз встретилось в continuation_counts\n",
        "        self.vocab = list(self.continuation_counts.keys())\n",
        "\n",
        "    def _recursive_prob(self, prefix_tuple, token):\n",
        "        \"\"\"\n",
        "        Рекурсивный расчёт P_KN(token | prefix_tuple).\n",
        "        order = len(prefix_tuple)+1\n",
        "        \"\"\"\n",
        "        order = len(prefix_tuple) + 1\n",
        "\n",
        "        # ---- база: униграммный Kneser–Ney\n",
        "        if order == 1:\n",
        "            return self.continuation_counts.get(token, 0) / self.cont_total\n",
        "\n",
        "        # основные счётчики\n",
        "        cnts = self.counts[order].get(prefix_tuple, Counter())\n",
        "        c_hw = cnts.get(token, 0)\n",
        "        c_h  = self.totals[order].get(prefix_tuple, 0)\n",
        "\n",
        "        # back-off префикс = суффикс (остальные n-2 слова)\n",
        "        backoff_prefix = prefix_tuple[1:] if len(prefix_tuple) > 1 else tuple()\n",
        "        p_backoff = self._recursive_prob(backoff_prefix, token)\n",
        "\n",
        "        # нормировочный коэффициент λ(h)\n",
        "        unique_h = self.uniques[order].get(prefix_tuple, 0)\n",
        "        λ = (self.delta * unique_h / c_h) if c_h > 0 else 0.0\n",
        "\n",
        "        # итоговая формула\n",
        "        p_kn = 0.0\n",
        "        if c_h > 0:\n",
        "            p_kn += max(c_hw - self.delta, 0) / c_h\n",
        "        p_kn += λ * p_backoff\n",
        "        return p_kn\n",
        "\n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        # приводим текстовый префикс к кортежу длины n-1\n",
        "        toks = prefix.split()\n",
        "        toks = toks[max(0, len(toks) - (self.n - 1)):]\n",
        "        toks = [UNK] * ((self.n - 1) - len(toks)) + toks\n",
        "        prefix_tuple = tuple(toks)\n",
        "        return self._recursive_prob(prefix_tuple, next_token)\n",
        "\n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "        # тот же приём преобразования prefix → tuple\n",
        "        toks = prefix.split()\n",
        "        toks = toks[max(0, len(toks) - (self.n - 1)):]\n",
        "        toks = [UNK] * ((self.n - 1) - len(toks)) + toks\n",
        "        prefix_tuple = tuple(toks)\n",
        "\n",
        "        # кандидаты: все, что есть в counts[n][prefix] + весь vocab\n",
        "        candidates = set(self.counts[self.n].get(prefix_tuple, {}).keys()) \\\n",
        "                   | set(self.vocab)\n",
        "\n",
        "        result = {}\n",
        "        for w in candidates:\n",
        "            p = self.get_next_token_prob(prefix, w)\n",
        "            if p > 0:\n",
        "                result[w] = p\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "cellId": "lsk91832qbmdt7x1q0a8z4",
        "id": "JS4gwQw4yOzZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Counting 1-grams: 100%|██████████| 100/100 [00:00<00:00, 129453.83it/s]\n",
            "Counting 1-grams: 100%|██████████| 100/100 [00:00<00:00, 75059.13it/s]\n",
            "Counting 2-grams: 100%|██████████| 100/100 [00:00<00:00, 39737.60it/s]\n",
            "Counting 1-grams: 100%|██████████| 100/100 [00:00<00:00, 135343.79it/s]\n",
            "Counting 2-grams: 100%|██████████| 100/100 [00:00<00:00, 68747.81it/s]\n",
            "Counting 3-grams: 100%|██████████| 100/100 [00:00<00:00, 57940.38it/s]\n"
          ]
        }
      ],
      "source": [
        "#test that it's a valid probability model\n",
        "for n in (1, 2, 3):\n",
        "    dummy_lm = KneserNeyLanguageModel(dummy_lines, n=n)\n",
        "    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "cellId": "pp3jtkk9annp1qkou58x1b",
        "id": "h5-4AxEtyOzZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Counting 1-grams: 100%|██████████| 30750/30750 [00:01<00:00, 26447.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N = 1, Perplexity = 1832.23136\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Counting 1-grams: 100%|██████████| 30750/30750 [00:01<00:00, 27218.58it/s]\n",
            "Counting 2-grams: 100%|██████████| 30750/30750 [00:02<00:00, 13323.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N = 2, Perplexity = 832.73936\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Counting 1-grams: 100%|██████████| 30750/30750 [00:01<00:00, 26676.73it/s]\n",
            "Counting 2-grams: 100%|██████████| 30750/30750 [00:02<00:00, 13466.06it/s]\n",
            "Counting 3-grams: 100%|██████████| 30750/30750 [00:04<00:00, 6471.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N = 3, Perplexity = 231976900.22803\n"
          ]
        }
      ],
      "source": [
        "for n in (1, 2, 3):\n",
        "    lm = KneserNeyLanguageModel(train_lines, n=n, delta=0.75)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "notebookId": "53997d2d-afb8-4477-8874-b6d46299f06c",
    "notebookPath": "seminar.ipynb"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
