{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmsFABwClrsS"
      },
      "source": [
        "## Seminar and homework\n",
        "\n",
        "Today we shall compose encoder-decoder neural networks and apply them to the task of machine translation.\n",
        "\n",
        "![img](https://esciencegroup.files.wordpress.com/2016/03/seq2seq.jpg)\n",
        "_(img: esciencegroup.files.wordpress.com)_\n",
        "\n",
        "\n",
        "Архитектуры encoder-decoder предназначены для преобразования чего угодно во что угодно, включая\n",
        " * Системы машинного перевода\n",
        " * [Генерация описания к изображению](http://mscoco.org/dataset/#captions-challenge2015) и [image2latex](https://openai.com/requests-for-research/#im2latex) (convolutional encoder, recurrent decoder)\n",
        " * Генерация [изображений по описанию](https://arxiv.org/abs/1511.02793) (рекуррентный кодер, конволюционный декодер)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4N9AD2dlrsU"
      },
      "source": [
        "## Наша задача: машинный перевод\n",
        "\n",
        "Мы собираемся опробовать наши модели кодировщика-декодировщика на задаче машинного перевода с русского на английский. В частности, мы будем переводить описания отелей и хостелов. Эта задача демонстрирует масштабность машинного перевода и при этом не требует недельного обучения модели, если не использовать GPU.\n",
        "\n",
        "Прежде чем мы перейдем к архитектуре, необходимо выполнить некоторую предварительную обработку. ~~Go tokenize~~ Хорошо, на этот раз мы выполнили предварительную обработку за вас. Как обычно, данные будут токенизированы с помощью WordPunctTokenizer.\n",
        "\n",
        "Однако есть еще одна вещь, которую нужно сделать. Наши строки данных содержат уникальные редкие слова. Если мы будем оперировать на уровне слов, нам придется иметь дело с большим объемом словарного запаса. Если же мы будем использовать модели на уровне символов, то обработка последовательности займет много итераций. В этот раз мы выберем что-то среднее.\n",
        "\n",
        "Один из популярных подходов называется [Byte Pair Encoding](https://github.com/rsennrich/subword-nmt), он же __BPE__, а еще лучше BBPE. Алгоритм начинает с токенизации на уровне символов, а затем итеративно объединяет наиболее частые пары в течение N итераций. В результате частые слова объединяются в одну лексему, а редкие слова разбиваются на слоги или даже символы.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfvojjHQlrsU",
        "outputId": "deef3d8b-7282-43ba-9ef1-b7375a785cef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-07-15 06:39:41--  https://www.dropbox.com/s/yy2zqh34dyhv07i/data.txt?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.70.18, 2620:100:6026:18::a27d:4612\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.70.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/mw8tdyetqboqwkn5ma886/data.txt?rlkey=t9fmsizx27ikh0vak0ir265a6&dl=1 [following]\n",
            "--2025-07-15 06:39:42--  https://www.dropbox.com/scl/fi/mw8tdyetqboqwkn5ma886/data.txt?rlkey=t9fmsizx27ikh0vak0ir265a6&dl=1\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucea8a937e9e1afba693a26060e4.dl.dropboxusercontent.com/cd/0/inline/Ctjd92bi00b2i_hv_H7k80W0yYL3DYv60piN73PBbRj7TXBDwVZoMbhqgoMbbrIcR9txVGJrNOUA_Wlq-Dikr96tVA3a0PO0jAVTSrq7k84-paBmAA8T_CdmtToOXqlagDQ/file?dl=1# [following]\n",
            "--2025-07-15 06:39:42--  https://ucea8a937e9e1afba693a26060e4.dl.dropboxusercontent.com/cd/0/inline/Ctjd92bi00b2i_hv_H7k80W0yYL3DYv60piN73PBbRj7TXBDwVZoMbhqgoMbbrIcR9txVGJrNOUA_Wlq-Dikr96tVA3a0PO0jAVTSrq7k84-paBmAA8T_CdmtToOXqlagDQ/file?dl=1\n",
            "Resolving ucea8a937e9e1afba693a26060e4.dl.dropboxusercontent.com (ucea8a937e9e1afba693a26060e4.dl.dropboxusercontent.com)... 162.125.70.15, 2620:100:6026:15::a27d:460f\n",
            "Connecting to ucea8a937e9e1afba693a26060e4.dl.dropboxusercontent.com (ucea8a937e9e1afba693a26060e4.dl.dropboxusercontent.com)|162.125.70.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12905335 (12M) [application/binary]\n",
            "Saving to: ‘data.txt’\n",
            "\n",
            "data.txt            100%[===================>]  12.31M  9.44MB/s    in 1.3s    \n",
            "\n",
            "2025-07-15 06:39:45 (9.44 MB/s) - ‘data.txt’ saved [12905335/12905335]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch>=1.3.0\n",
        "!pip3 install subword-nmt &> log\n",
        "!wget https://www.dropbox.com/s/yy2zqh34dyhv07i/data.txt?dl=1 -O data.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7wK0SpHF10Nt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Response [404]>\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'href'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m response = requests.get(final_url)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m download_url = \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhref\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Загружаем файл и сохраняем его\u001b[39;00m\n\u001b[32m     14\u001b[39m download_response = requests.get(download_url)\n",
            "\u001b[31mKeyError\u001b[39m: 'href'"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from urllib.parse import urlencode\n",
        "\n",
        "base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n",
        "public_key = 'https://disk.yandex.ru/d/d_dZfYoS4o4ckQ'\n",
        "# thanks to tilda and deephack teams for the data, Dmitry Emelyanenko for the code :)\n",
        "# Получаем загрузочную ссылку\n",
        "final_url = base_url + urlencode(dict(public_key=public_key))\n",
        "response = requests.get(final_url)\n",
        "print(response)\n",
        "download_url = response.json()['href']\n",
        "\n",
        "# Загружаем файл и сохраняем его\n",
        "download_response = requests.get(download_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "p2ST80od38M6"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'download_response' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m./vocab.py\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     f.write(\u001b[43mdownload_response\u001b[49m.content)\n",
            "\u001b[31mNameError\u001b[39m: name 'download_response' is not defined"
          ]
        }
      ],
      "source": [
        "with open('./vocab.py', 'wb') as f:\n",
        "    f.write(download_response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9kP0SdxlrsY",
        "outputId": "7abfd56a-9122-440d-e6e6-74178e1f13a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8000/8000 [00:03<00:00, 2108.38it/s]\n",
            "100%|██████████| 8000/8000 [00:03<00:00, 2039.09it/s]\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from subword_nmt.learn_bpe import learn_bpe\n",
        "from subword_nmt.apply_bpe import BPE\n",
        "tokenizer = WordPunctTokenizer()\n",
        "def tokenize(x):\n",
        "    return ' '.join(tokenizer.tokenize(x.lower()))\n",
        "\n",
        "# split and tokenize the data\n",
        "with open('train.en', 'w') as f_src,  open('train.ru', 'w') as f_dst:\n",
        "    for line in open('data.txt'):\n",
        "        src_line, dst_line = line.strip().split('\\t')\n",
        "        f_src.write(tokenize(src_line) + '\\n')\n",
        "        f_dst.write(tokenize(dst_line) + '\\n')\n",
        "\n",
        "# build and apply bpe vocs\n",
        "bpe = {}\n",
        "for lang in ['en', 'ru']:\n",
        "    learn_bpe(open('./train.' + lang), open('bpe_rules.' + lang, 'w'), num_symbols=8000)\n",
        "    bpe[lang] = BPE(open('./bpe_rules.' + lang))\n",
        "\n",
        "    with open('train.bpe.' + lang, 'w') as f_out:\n",
        "        for line in open('train.' + lang):\n",
        "            f_out.write(bpe[lang].process_line(line.strip()) + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UPW3sV8lrsb"
      },
      "source": [
        "### Building vocabularies\n",
        "\n",
        "We now need to build vocabularies that map strings to token ids and vice versa. We're gonna need these fellas when we feed training data into model or convert output matrices into words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CmTy_m_olrsb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PskgBSxlrsd",
        "outputId": "767d5b04-0d4a-420a-be2c-d47526e7259c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inp: на территории обустроена бесплатная частная парковка .\n",
            "out: free private parking is available on site .\n",
            "\n",
            "inp: кроме того , в 5 минутах ходьбы работают многочисленные бары и рестораны .\n",
            "out: guests can find many bars and restaurants within a 5 - minute walk .\n",
            "\n",
            "inp: отель san mi@@ gu@@ el расположен в центре мор@@ ели@@ и , в 750 метрах от главной площади города и кафедрального собора .\n",
            "out: hotel san miguel is located in central more@@ lia , 750 metres from the city ’ s main square and cathedral .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data_inp = np.array(open('./train.bpe.ru').read().split('\\n'))\n",
        "data_out = np.array(open('./train.bpe.en').read().split('\\n'))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_inp, dev_inp, train_out, dev_out = train_test_split(data_inp, data_out, test_size=3000,\n",
        "                                                          random_state=42)\n",
        "for i in range(3):\n",
        "    print('inp:', train_inp[i])\n",
        "    print('out:', train_out[i], end='\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vipg4O61lrsg"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'Vocab' from 'vocab' (/home/german/nlp/4_HW/vocab.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvocab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Vocab\n\u001b[32m      2\u001b[39m inp_voc = Vocab.from_lines(train_inp)\n\u001b[32m      3\u001b[39m out_voc = Vocab.from_lines(train_out)\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'Vocab' from 'vocab' (/home/german/nlp/4_HW/vocab.py)"
          ]
        }
      ],
      "source": [
        "from vocab import Vocab\n",
        "inp_voc = Vocab.from_lines(train_inp)\n",
        "out_voc = Vocab.from_lines(train_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwOoHfuhlrsi",
        "outputId": "712bfb8d-58a6-4704-8aee-ae0b29f5e615"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'inp_voc' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Here's how you cast lines into ids and backwards.\u001b[39;00m\n\u001b[32m      2\u001b[39m batch_lines = \u001b[38;5;28msorted\u001b[39m(train_inp, key=\u001b[38;5;28mlen\u001b[39m)[\u001b[32m5\u001b[39m:\u001b[32m10\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m batch_ids = \u001b[43minp_voc\u001b[49m.to_matrix(batch_lines)\n\u001b[32m      4\u001b[39m batch_lines_restored = inp_voc.to_lines(batch_ids)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlines\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'inp_voc' is not defined"
          ]
        }
      ],
      "source": [
        "# Here's how you cast lines into ids and backwards.\n",
        "batch_lines = sorted(train_inp, key=len)[5:10]\n",
        "batch_ids = inp_voc.to_matrix(batch_lines)\n",
        "batch_lines_restored = inp_voc.to_lines(batch_ids)\n",
        "\n",
        "print(\"lines\")\n",
        "print(batch_lines)\n",
        "print(\"\\nwords to ids (0 = bos, 1 = eos):\")\n",
        "print(batch_ids)\n",
        "print(\"\\nback to words\")\n",
        "print(batch_lines_restored)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSYu-MkElrsk"
      },
      "source": [
        "Draw source and translation length distributions to estimate the scope of the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLLl9cSNlrsl"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[8, 4])\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"source length\")\n",
        "plt.hist(list(map(len, map(str.split, train_inp))), bins=20);\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"translation length\")\n",
        "plt.hist(list(map(len, map(str.split, train_out))), bins=20);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHWgx34flrsn"
      },
      "source": [
        "### Encoder-decoder model\n",
        "\n",
        "Код ниже содержит шаблон для простой модели кодера-декодера: один GRU-кодер/декодер, никакого внимания или чего-либо еще. Чистый BASELINE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd_rDRm9lrso"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgfN5-F7lrst"
      },
      "outputs": [],
      "source": [
        "class BasicModel(nn.Module):\n",
        "    def __init__(self, inp_voc, out_voc, emb_size=64, hid_size=128):\n",
        "        \"\"\"\n",
        "        A simple encoder-decoder seq2seq model\n",
        "        \"\"\"\n",
        "        super().__init__() # initialize base class to track sub-layers, parameters, etc.\n",
        "\n",
        "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
        "        self.hid_size = hid_size\n",
        "\n",
        "        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)\n",
        "        self.emb_out = nn.Embedding(len(out_voc), emb_size)\n",
        "        self.enc0 = nn.GRU(emb_size, hid_size, batch_first=True)\n",
        "\n",
        "        self.dec_start = nn.Linear(hid_size, hid_size)\n",
        "        self.dec0 = nn.GRUCell(emb_size, hid_size)\n",
        "        self.logits = nn.Linear(hid_size, len(out_voc))\n",
        "\n",
        "    def forward(self, inp, out):\n",
        "        \"\"\" Apply model in training mode \"\"\"\n",
        "        initial_state = self.encode(inp)\n",
        "        return self.decode(initial_state, out)\n",
        "\n",
        "\n",
        "    def encode(self, inp, **flags):\n",
        "        \"\"\"\n",
        "        Takes symbolic input sequence, computes initial state\n",
        "        :param inp: matrix of input tokens [batch, time]\n",
        "        :returns: initial decoder state tensors, one or many\n",
        "        \"\"\"\n",
        "        inp_emb = self.emb_inp(inp)\n",
        "        batch_size = inp.shape[0]\n",
        "\n",
        "        enc_seq, [last_state_but_not_really] = self.enc0(inp_emb)\n",
        "        # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]\n",
        "\n",
        "        # note: last_state is not _actually_ last because of padding, let's find the real last_state\n",
        "        lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)\n",
        "        last_state = enc_seq[torch.arange(len(enc_seq)), lengths]\n",
        "        # ^-- shape: [batch_size, hid_size]\n",
        "\n",
        "        dec_start = self.dec_start(last_state)\n",
        "        return [dec_start]\n",
        "\n",
        "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
        "        \"\"\"\n",
        "        :param prev_state: [hidden], где hidden — тензор [batch, hid_size]\n",
        "        :param prev_tokens: тензор [batch] с индексами предыдущих токенов\n",
        "        :returns: new_state, output_logits\n",
        "        \"\"\"\n",
        "        # 1) достаём предыдущее состояние GRUCell\n",
        "        prev_hidden = prev_state[0]  # тензор [batch, hid_size]\n",
        "\n",
        "        # 2) эмбеддим предыдущие выходные токены\n",
        "        prev_emb = self.emb_out(prev_tokens)  # [batch, emb_size]\n",
        "\n",
        "        # 3) один шаг GRUCell\n",
        "        new_hidden = self.dec0(prev_emb, prev_hidden)  # [batch, hid_size]\n",
        "\n",
        "        # 4) вычисляем логиты для следующего токена\n",
        "        output_logits = self.logits(new_hidden)  # [batch, len(out_voc)]\n",
        "\n",
        "        # 5) упаковываем новое состояние в список, как и initial_state\n",
        "        new_state = [new_hidden]\n",
        "\n",
        "        return new_state, output_logits\n",
        "    \n",
        "    def decode(self, initial_state, out_tokens, **flags):\n",
        "        \"\"\" Iterate over reference tokens (out_tokens) with decode_step \"\"\"\n",
        "        batch_size = out_tokens.shape[0]\n",
        "        state = initial_state\n",
        "\n",
        "        # initial logits: always predict BOS\n",
        "        onehot_bos = F.one_hot(torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64),\n",
        "                               num_classes=len(self.out_voc)).to(device=out_tokens.device)\n",
        "        first_logits = torch.log(onehot_bos.to(torch.float32) + 1e-9)\n",
        "\n",
        "        logits_sequence = [first_logits]\n",
        "        for i in range(out_tokens.shape[1] - 1):\n",
        "            state, logits = self.decode_step(state, out_tokens[:, i])\n",
        "            logits_sequence.append(logits)\n",
        "        return torch.stack(logits_sequence, dim=1)\n",
        "\n",
        "    def decode_inference(self, initial_state, max_len=100, **flags):\n",
        "        \"\"\" Generate translations from model (greedy version) \"\"\"\n",
        "        batch_size, device = len(initial_state[0]), initial_state[0].device\n",
        "        state = initial_state\n",
        "        outputs = [torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64,\n",
        "                              device=device)]\n",
        "        all_states = [initial_state]\n",
        "\n",
        "        for i in range(max_len):\n",
        "            state, logits = self.decode_step(state, outputs[-1])\n",
        "            outputs.append(logits.argmax(dim=-1))\n",
        "            all_states.append(state)\n",
        "\n",
        "        return torch.stack(outputs, dim=1), all_states\n",
        "\n",
        "    def translate_lines(self, inp_lines, **kwargs):\n",
        "        inp = self.inp_voc.to_matrix(inp_lines).to(device)\n",
        "        initial_state = self.encode(inp)\n",
        "        out_ids, states = self.decode_inference(initial_state, **kwargs)\n",
        "        return self.out_voc.to_lines(out_ids.cpu().numpy()), states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNERMcKzglbY"
      },
      "outputs": [],
      "source": [
        "# debugging area\n",
        "model = BasicModel(inp_voc, out_voc).to(device)\n",
        "\n",
        "dummy_inp_tokens = inp_voc.to_matrix(sorted(train_inp, key=len)[5:10]).to(device)\n",
        "dummy_out_tokens = out_voc.to_matrix(sorted(train_out, key=len)[5:10]).to(device)\n",
        "\n",
        "h0 = model.encode(dummy_inp_tokens)\n",
        "h1, logits1 = model.decode_step(h0, torch.arange(len(dummy_inp_tokens), device=device))\n",
        "\n",
        "assert isinstance(h1, list) and len(h1) == len(h0)\n",
        "assert h1[0].shape == h0[0].shape and not torch.allclose(h1[0], h0[0])\n",
        "assert logits1.shape == (len(dummy_inp_tokens), len(out_voc))\n",
        "\n",
        "logits_seq = model.decode(h0, dummy_out_tokens)\n",
        "assert logits_seq.shape == (dummy_out_tokens.shape[0], dummy_out_tokens.shape[1], len(out_voc))\n",
        "\n",
        "# full forward\n",
        "logits_seq2 = model(dummy_inp_tokens, dummy_out_tokens)\n",
        "assert logits_seq2.shape == logits_seq.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBcUZpC9glbY"
      },
      "outputs": [],
      "source": [
        "dummy_translations, dummy_states = model.translate_lines(train_inp[:3], max_len=25)\n",
        "print(\"Translations without training:\")\n",
        "print('\\n'.join([line for line in dummy_translations]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wuv1-aVlrs0"
      },
      "source": [
        "### Compute loss\n",
        "\n",
        "Наша цель обучения почти такая же, как и для LM:\n",
        "\n",
        "$$ L = {\\frac1{|D|}} \\sum_{X, Y \\in D} \\sum_{y_t \\in Y} - \\log p(y_t \\mid y_1, \\dots, y_{t-1}, X, \\theta) $$\n",
        "\n",
        "где $|D|$ - _общая длина всех последовательностей__, включая BOS и первый EOS, но исключая PAD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8XPV8sWlrs5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_loss(model, inp, out, **flags):\n",
        "    \"\"\"\n",
        "    Compute loss (float32 scalar) as in the formula above\n",
        "    :param inp: input tokens matrix, int32[batch, time]\n",
        "    :param out: reference tokens matrix, int32[batch, time]\n",
        "\n",
        "    In order to pass the tests, your function should\n",
        "    * include loss at first EOS but not the subsequent ones\n",
        "    * divide sum of losses by a sum of input lengths (use voc.compute_mask)\n",
        "    \"\"\"\n",
        "    # mask == 1 for all positions up to and including first EOS, 0 afterwards\n",
        "    mask = model.out_voc.compute_mask(out).to(torch.float32)  # [B, T]\n",
        "\n",
        "    # [B, T, V]\n",
        "    logits_seq = model(inp, out)\n",
        "\n",
        "    # [B, T, V]\n",
        "    logprobs_seq = F.log_softmax(logits_seq, dim=-1)\n",
        "\n",
        "    # [B, T] log-probabilities of the true tokens\n",
        "    # note: F.one_hot(out, V) has same shape as logprobs_seq, picks out correct log-prob\n",
        "    targets_1hot = F.one_hot(out, num_classes=logits_seq.size(-1)).to(torch.float32)\n",
        "    logp_out = (logprobs_seq * targets_1hot).sum(dim=-1)\n",
        "\n",
        "    # sum negative log-probs over valid positions and normalize by total valid tokens\n",
        "    loss = - (logp_out * mask).sum() / mask.sum()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME_LWUeklrs7"
      },
      "outputs": [],
      "source": [
        "dummy_loss = compute_loss(model, dummy_inp_tokens, dummy_out_tokens)\n",
        "print(\"Loss:\", dummy_loss)\n",
        "assert np.allclose(dummy_loss.item(), 7.5, rtol=0.1, atol=0.1), \"We're sorry for your loss\"\n",
        "\n",
        "# test autograd\n",
        "dummy_loss.backward()\n",
        "for name, param in model.named_parameters():\n",
        "    assert param.grad is not None and abs(param.grad.max()) != 0, f\"Param {name} received no gradients\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpbaBpW7lrs-"
      },
      "source": [
        "### Оценка: BLEU\n",
        "\n",
        "Машинный перевод обычно оценивается с помощью оценки [BLEU](https://en.wikipedia.org/wiki/BLEU). Эта метрика просто вычисляет, какая доля предсказанных n-грамм действительно присутствует в эталонном переводе. Она делает это для n=1,2,3 и 4 и вычисляет среднее геометрическое со штрафом, если перевод короче эталонного.\n",
        "\n",
        "Хотя у BLEU [есть много недостатков] (http://www.cs.jhu.edu/~ccb/publications/re-evaluating-the-role-of-bleu-in-mt-research.pdf), она по-прежнему остается наиболее часто используемой метрикой и одной из самых простых в вычислении."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gb1-PhKIlrs-"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "def compute_bleu(model, inp_lines, out_lines, bpe_sep='@@ ', **flags):\n",
        "    \"\"\"\n",
        "    Estimates corpora-level BLEU score of model's translations given inp and reference out\n",
        "    Note: if you're serious about reporting your results, use https://pypi.org/project/sacrebleu\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        translations, _ = model.translate_lines(inp_lines, **flags)\n",
        "        translations = [line.replace(bpe_sep, '') for line in translations]\n",
        "        actual = [line.replace(bpe_sep, '') for line in out_lines]\n",
        "        return corpus_bleu(\n",
        "            [[ref.split()] for ref in actual],\n",
        "            [trans.split() for trans in translations],\n",
        "            smoothing_function=lambda precisions, **kw: [p + 1.0 / p.denominator for p in precisions]\n",
        "            ) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZvfid1RlrtA"
      },
      "outputs": [],
      "source": [
        "compute_bleu(model, dev_inp, dev_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQDhGwg4lrtC"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "Training encoder-decoder models isn't that different from any other models: sample batches, compute loss, backprop and update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfwIaixHlrtI",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm, trange\n",
        "metrics = {'train_loss': [], 'dev_bleu': [] }\n",
        "\n",
        "model = BasicModel(inp_voc, out_voc).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlDT6eDUlrtL",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "for _ in trange(25000):\n",
        "    step = len(metrics['train_loss']) + 1\n",
        "    batch_ix = np.random.randint(len(train_inp), size=batch_size)\n",
        "    batch_inp = inp_voc.to_matrix(train_inp[batch_ix]).to(device)\n",
        "    batch_out = out_voc.to_matrix(train_out[batch_ix]).to(device)\n",
        "\n",
        "    # Переводим модель в режим тренировки (на случай, если вы где-то переключали)\n",
        "    model.train()\n",
        "\n",
        "    # Нулируем градиенты\n",
        "    opt.zero_grad()\n",
        "\n",
        "    # Считаем лосс на батче\n",
        "    loss_t = compute_loss(model, batch_inp, batch_out)\n",
        "\n",
        "    # Обратное распространение ошибки\n",
        "    loss_t.backward()\n",
        "\n",
        "    # Шаг оптимизатора\n",
        "    opt.step()\n",
        "\n",
        "    metrics['train_loss'].append((step, loss_t.item()))\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        metrics['dev_bleu'].append((step, compute_bleu(model, dev_inp, dev_out)))\n",
        "\n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=(12,4))\n",
        "        for i, (name, history) in enumerate(sorted(metrics.items())):\n",
        "            plt.subplot(1, len(metrics), i + 1)\n",
        "            plt.title(name)\n",
        "            plt.plot(*zip(*history))\n",
        "            plt.grid()\n",
        "        plt.show()\n",
        "        print(\"Mean loss=%.3f\" % np.mean(metrics['train_loss'][-10:], axis=0)[1], flush=True)\n",
        "\n",
        "# Note: it's okay if bleu oscillates up and down as long as it gets better on average over long term (e.g. 5k batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ahuhKVhlrtP"
      },
      "outputs": [],
      "source": [
        "assert np.mean(metrics['dev_bleu'][-10:], axis=0)[1] > 15, \"We kind of need a higher bleu BLEU from you. Kind of right now.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyaHOpealrtS"
      },
      "outputs": [],
      "source": [
        "for inp_line, trans_line in zip(dev_inp[::500], model.translate_lines(dev_inp[::500])[0]):\n",
        "    print(inp_line)\n",
        "    print(trans_line)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edk_oVg0lrtW"
      },
      "source": [
        "### Требуется Ваше Внимание xD\n",
        "\n",
        "В этом разделе мы хотим, чтобы вы улучшили базовую модель, реализовав простой механизм внимания.\n",
        "\n",
        "Это будет две задачи: создание  слоя __attention__ и его использование для  модели __attention seq2seq__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz9aROAIlrtX"
      },
      "source": [
        "### Attention layer\n",
        "\n",
        "Здесь вам предстоит реализовать слой, который вычисляет simple addition attention:\n",
        "Дана последовательность енкодеров $h^e_0, h^e_1, h^e_2, ..., h^e_T$ и одно состояние декодера $h^d$,\n",
        "* Вычислите логиты с помощью двухслойной нейронной сети\n",
        "$$a_t= linear_{out}(tanh(linear_{e}(h^e_t) + linear_{d}(h_d)))$$\n",
        "* Получение \"вероятностей\" из логитов,\n",
        "$$ p_t = {{e ^ {a_t}} \\over { \\sum_\\tau e^{a_\\tau} }} $$\n",
        "* Сложите состояния кодировщика с вероятностями, чтобы получить __attention response__.\n",
        "$$ attn = \\sum_t p_t \\cdot h^e_t $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1WOmPcDglbZ"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, name, enc_size, dec_size, hid_size, activ=torch.tanh):\n",
        "        \"\"\" A layer that computes additive attention response and weights \"\"\"\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.enc_size = enc_size # num units in encoder state\n",
        "        self.dec_size = dec_size # num units in decoder state\n",
        "        self.hid_size = hid_size # attention layer hidden units\n",
        "        self.activ = activ       # attention layer hidden nonlinearity\n",
        "\n",
        "        # create trainable paramteres like this:\n",
        "        self.W_e = nn.Parameter(torch.randn(hid_size, enc_size) * (1.0 / enc_size**0.5), requires_grad=True)\n",
        "        self.W_d = nn.Parameter(torch.randn(hid_size, enc_size) * (1.0 / enc_size**0.5), requires_grad=True)\n",
        "        self.v = nn.Parameter(torch.randn(hid_size, enc_size) * (1.0 / enc_size**0.5), requires_grad=True)  # you will need a couple of these\n",
        "\n",
        "\n",
        "    def forward(self, enc, dec, inp_mask):\n",
        "        \"\"\"\n",
        "        Computes attention response and weights\n",
        "        :param enc: encoder activation sequence, float32[batch_size, ninp, enc_size]\n",
        "        :param dec: single decoder state used as \"query\", float32[batch_size, dec_size]\n",
        "        :param inp_mask: mask on enc activatons (0 after first eos), float32 [batch_size, ninp]\n",
        "        :returns: attn[batch_size, enc_size], probs[batch_size, ninp]\n",
        "            - attn - attention response vector (weighted sum of enc)\n",
        "            - probs - attention weights after softmax\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute logits\n",
        "        enc_proj = torch.matmul(enc, self.W_e.t())\n",
        "\n",
        "        #    dec_proj: [B, hid_size] -> [B, 1, hid_size] -> [B, T, hid_size]\n",
        "        dec_proj = torch.matmul(dec, self.W_d.t()).unsqueeze(1)\n",
        "        \n",
        "\n",
        "        #    u: [B, T, hid_size]\n",
        "        u = self.activ(enc_proj + dec_proj)\n",
        "\n",
        "\n",
        "        logits = torch.matmul(u, self.v)\n",
        "        # Apply mask - if mask is 0, logits should be -inf or -1e9\n",
        "        # You may need torch.where\n",
        "        mask_bool = inp_mask.to(torch.bool)\n",
        "\n",
        "        logits_masked = torch.where(mask_bool, logits, -1e9)\n",
        "\n",
        "\n",
        "\n",
        "        # Compute attention probabilities (softmax)\n",
        "        probs = F.softmax(logits_masked, dim=1)\n",
        "\n",
        "        # Compute attention response using enc and probs\n",
        "        attn = torch.bmm(probs.unsqueeze(1), enc).squeeze(1)\n",
        "\n",
        "        return attn, probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IalfpdAelrtb"
      },
      "source": [
        "### Seq2seq model with attention (3 points)\n",
        "\n",
        "Теперь вы можете использовать слой внимания для построения сети. Самый простой способ реализовать внимание - использовать его на этапе декодера:\n",
        "![img](https://i.imgur.com/6fKHlHb.png)\n",
        "_image from distill.pub [article](https://distill.pub/2016/augmented-rnns/)_\n",
        "\n",
        "На каждом шаге используйте состояние _?_предыдущего__ декодера для рассчета attention. затем конкатим и передаем далее на вход.\n",
        "\n",
        "Ключевой деталью реализации здесь является __model state__. Проще говоря, вы можете добавить любой тензор в список выходов `encode`. Затем вы будете иметь к ним доступ на каждом шаге `декодирования`. К ним могут относиться:\n",
        "* Последние скрытые состояния RNN (как в базовой модели)\n",
        "* Всю последовательность выходов кодuировщика (на которые нужно обратить внимание) и маску\n",
        "* Вероятности внимания (для визуализацhbии)\n",
        "\n",
        "_There are, of course, alternative ways to wire attention into your network and different kinds of attention. Take a look at [this](https://arxiv.org/abs/1609.08144), [this](https://arxiv.org/abs/1706.03762) and [this](https://arxiv.org/abs/1808.03867) for ideas. And for image captioning/im2latex there's [visual attention](https://arxiv.org/abs/1502.03044)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCKPB5JmcE6j"
      },
      "outputs": [],
      "source": [
        "class AttentiveModel(BasicModel):\n",
        "    def __init__(self, name, inp_voc, out_voc,\n",
        "                 emb_size=64, hid_size=128, attn_size=128):\n",
        "        \"\"\" Translation model that uses attention. See instructions above. \"\"\"\n",
        "        nn.Module.__init__(self)  # initialize base class to track sub-layers, trainable variables, etc.\n",
        "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
        "        self.hid_size = hid_size\n",
        "\n",
        "         # (A) Сохраним имя\n",
        "        self.name = name\n",
        "\n",
        "        # (B) Слой внимания: encoder→decoder\n",
        "        # enc_size = hid_size (выход GRU в BasicModel), dec_size = hid_size\n",
        "        self.attention = AttentionLayer(\n",
        "            name + \"_attn\",\n",
        "            enc_size=hid_size,\n",
        "            dec_size=hid_size,\n",
        "            hid_size=attn_size\n",
        "        )\n",
        "\n",
        "    def encode(self, inp, **flags):\n",
        "        \"\"\"\n",
        "        Takes symbolic input sequence, computes initial state\n",
        "        :param inp: matrix of input tokens [batch, time]\n",
        "        :return: a list of initial decoder state tensors\n",
        "        \"\"\"\n",
        "\n",
        "        inp_emb = self.emb_inp(inp)                   # [B, T, emb]\n",
        "        enc_seq, _ = self.enc0(inp_emb)               # [B, T, hid]\n",
        "        # находим реальные lengths и последний hidden, как в BasicModel:\n",
        "        lengths = (inp != self.inp_voc.eos_ix).sum(1).clamp_max(inp.size(1)-1)\n",
        "        last_state = enc_seq[torch.arange(inp.size(0)), lengths]  # [B, hid]\n",
        "        dec0_state = self.dec_start(last_state)        # [B, hid]\n",
        "\n",
        "        # 2) маска для encoder (1 до первого EOS включительно, иначе 0)\n",
        "        inp_mask = self.inp_voc.compute_mask(inp)      # [B, T]\n",
        "\n",
        "        # 3) первый вес внимания (из initial decoder state)\n",
        "        #    передаём ВСЁ enc_seq и текущее dec0_state\n",
        "        first_attn_probs = self.attention(enc_seq, dec0_state, inp_mask)[1]  # [B, T]\n",
        "\n",
        "        # Build first state: include\n",
        "        # * initial states for decoder recurrent layers\n",
        "        # * encoder sequence and encoder attn mask (for attention)\n",
        "        # * make sure that last state item is attention probabilities tensor\n",
        "\n",
        "        first_state = [dec0_state, enc_seq, inp_mask, first_attn_probs]\n",
        "        return first_state\n",
        "\n",
        "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
        "        \"\"\"\n",
        "        Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
        "        :param prev_state: a list of previous decoder state tensors\n",
        "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
        "        :return: a list of next decoder state tensors, a tensor of logits [batch, n_tokens]\n",
        "        \"\"\"\n",
        "\n",
        "        # prev_state = [dec_hidden, enc_seq, inp_mask, prev_attn_probs]\n",
        "        dec_hidden, enc_seq, inp_mask, _ = prev_state\n",
        "\n",
        "        # 1) эмбеддинг предыдущего выхода\n",
        "        prev_emb = self.emb_out(prev_tokens)         # [B, emb]\n",
        "\n",
        "        # 2) допустим, мы хотим включить в decoder input сам attention response:\n",
        "        #    получаем attn_vec и новые веса:\n",
        "        attn_vec, attn_probs = self.attention(enc_seq, dec_hidden, inp_mask)  # ([B, hid], [B, T])\n",
        "\n",
        "        # 3) склеиваем prev_emb и attn_vec, чтобы дать декодеру контекст\n",
        "        dec_input = torch.cat([prev_emb, attn_vec], dim=1)  # [B, emb+hid]\n",
        "\n",
        "        # 4) один шаг GRUCell\n",
        "        new_hidden = self.dec0(dec_input, dec_hidden)      # [B, hid]\n",
        "\n",
        "        # 5) логиты на следующем шаге\n",
        "        output_logits = self.logits(new_hidden)             # [B, V_out]\n",
        "\n",
        "        # 6) новый state: аналогично encode\n",
        "        new_state = [new_hidden, enc_seq, inp_mask, attn_probs]\n",
        "\n",
        "        return [new_state, output_logits]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryZCOTEslrtf"
      },
      "source": [
        "### Training attentive model\n",
        "\n",
        "просто архитектура обычной модели, избегайте сложностей))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YMHPgZxcFaQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AttentiveModel(\"attn\", inp_voc, out_voc,\n",
        "                       emb_size=64, hid_size=128, attn_size=128).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIu0RZEsglbb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP1o9gx4glbb"
      },
      "outputs": [],
      "source": [
        "<YOUR CODE: measure final BLEU>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpb8BFqLglbb"
      },
      "source": [
        "### Визуализация внимания модели (2 балла)\n",
        "После обучения модели внимательного перевода вы можете проверить ее работоспособность, визуализировав веса внимания.\n",
        "\n",
        "Попробуй проинтепретировать их\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wt4YmECIglbb"
      },
      "outputs": [],
      "source": [
        "import bokeh.plotting as pl\n",
        "import bokeh.models as bm\n",
        "from bokeh.io import output_notebook, show\n",
        "output_notebook()\n",
        "\n",
        "def draw_attention(inp_line, translation, probs):\n",
        "    \"\"\" An intentionally ambiguous function to visualize attention weights \"\"\"\n",
        "    inp_tokens = inp_voc.tokenize(inp_line)\n",
        "    trans_tokens = out_voc.tokenize(translation)\n",
        "    probs = probs[:len(trans_tokens), :len(inp_tokens)]\n",
        "\n",
        "    fig = pl.figure(x_range=(0, len(inp_tokens)), y_range=(0, len(trans_tokens)),\n",
        "                    x_axis_type=None, y_axis_type=None, tools=[])\n",
        "    fig.image([probs[::-1]], 0, 0, len(inp_tokens), len(trans_tokens))\n",
        "\n",
        "    fig.add_layout(bm.LinearAxis(axis_label='source tokens'), 'above')\n",
        "    fig.xaxis.ticker = np.arange(len(inp_tokens)) + 0.5\n",
        "    fig.xaxis.major_label_overrides = dict(zip(np.arange(len(inp_tokens)) + 0.5, inp_tokens))\n",
        "    fig.xaxis.major_label_orientation = 45\n",
        "\n",
        "    fig.add_layout(bm.LinearAxis(axis_label='translation tokens'), 'left')\n",
        "    fig.yaxis.ticker = np.arange(len(trans_tokens)) + 0.5\n",
        "    fig.yaxis.major_label_overrides = dict(zip(np.arange(len(trans_tokens)) + 0.5, trans_tokens[::-1]))\n",
        "\n",
        "    show(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbArbGtZglbb"
      },
      "outputs": [],
      "source": [
        "inp = dev_inp[::500]\n",
        "\n",
        "trans, states = model.translate_lines(inp)\n",
        "\n",
        "# select attention probs from model state (you may need to change this for your custom model)\n",
        "# attention_probs below must have shape [batch_size, translation_length, input_length], extracted from states\n",
        "# e.g. if attention probs are at the end of each state, use np.stack([state[-1] for state in states], axis=1)\n",
        "attention_probs = <YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2ap5f4nglbc"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    draw_attention(inp[i], trans[i], attention_probs[i])\n",
        "\n",
        "# Does it look fine already? don't forget to save images for anytask!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbIIngNVlrtt"
      },
      "source": [
        "## Goind deeper (2++ балла)\n",
        "\n",
        "А теперь реши эту задачу с учетом всего, что ты знаешь:\n",
        "\n",
        "1) различные lstm / rnn / gru\n",
        "\n",
        "2) bidirectional encoder, different attention methods for decoder (additive, dot-product, multi-head)\n",
        "\n",
        "3) word dropout, training schedules, layernorm / batchnorm\n",
        "\n",
        "4) долой greedy decoding, делаем beam-search!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "2rzAj_xtlrtt"
      },
      "source": [
        "`Напиши добротный, подробный отчет о своих результатах`"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "edk_oVg0lrtW"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
